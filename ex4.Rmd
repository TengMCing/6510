---
title: "ex4"
output: 
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Question 1

## (1)

The likelihood for $\theta$ is $$L(\theta:y_1,...y_n) = K(y_1,...,y_n) \times f(y_1,...,y_n:\theta)=K(y_1,...,y_n) \times \prod_{i=1}^{n}\theta y_i^{\theta-1} exp(-y_i^\theta).$$

The support for $\theta$ is $$logL(\theta:y_1,...y_n) = logK(y_1,...,y_n) \times \sum_{i=1}^{n}log(\theta) + (\theta-1)log(y_i) - y_i^\theta.$$

The score function for $\theta$ is $$\boldsymbol{S}(\theta:y_1,...,y_n) = \frac{\partial logL(\theta:y_1,...y_n)}{\partial \theta} = \sum_{i=1}^{n}\frac{1}{\theta} + log(y_i) - y_i^\theta log(y_i).$$

## (2)

The MLE can be obtained by solving the first order condition, 

$$\boldsymbol{S}(\theta:y_1,...,y_n) = \frac{\partial logL(\theta:y_1,...y_n)}{\partial \theta} = \sum_{i=1}^{n}\frac{1}{\theta} + log(y_i) - y_i^\theta log(y_i) = 0.$$

However, there is no closed-form solution.

## (3)

In practical situation, I would use gradient descent to find the maximum.


# Question 2

## (1)

The log likelihood function is

\begin{align*}
logL(\boldsymbol{\theta}:y_1,...y_n) &= logK(y_1,..,y_n) + logf(y1,...,y_n:\boldsymbol{\theta})\\
&=k+log\left[\prod_{i=1}^{n}\frac{1}{\sqrt{2\pi\sigma^2}}exp\left\{-\frac{(y_i-\mu)^2}{2\sigma^2}\right\}\right]\\
&=k+\sum_{i=1}^{n}-\frac{1}{2}log(2\pi\sigma^2)-\frac{(y_i-\mu)^2}{2\sigma^2}\\
&=k-\frac{n}{2}log(2\pi\sigma^2)-\sum_{i=1}^{n}\frac{(y_i-\mu)^2}{2\sigma^2}\\
&=k-\frac{n}{2}log(\sigma^2)-\sum_{i=1}^{n}\frac{1}{2}\left(\frac{y_i-\mu}{\sigma}\right)^2.
\end{align*}

## (2)

The Kullback-Liebler divergence is


\begin{align*}
K_n(\boldsymbol{\theta}:\boldsymbol{\theta}_0)&=nK(\boldsymbol{\theta}:\boldsymbol{\theta}_0)\\
&=nE\left[log\left\{\frac{f(y:\boldsymbol{\theta}_0)}{f(y:\boldsymbol{\theta})}\right\}\right]\\
&=nE\left[log\{f(y:\boldsymbol{\theta})\}-log\{f(y:\boldsymbol{\theta}_0)\}\right]\\
&=nE\left[k-\frac{1}{2}log(\sigma_0^2)-\frac{1}{2}\left(\frac{y-\mu_0}{\sigma_0}\right)^2-k+\frac{1}{2}log(\sigma^2)+\frac{1}{2}\left(\frac{y-\mu}{\sigma}\right)^2\right]\\
&=nE\left[log\left(\frac{\sigma}{\sigma_0}\right)-\frac{1}{2}\left(\frac{y-\mu_0}{\sigma_0}\right)^2+\frac{1}{2}\left(\frac{y-\mu}{\sigma}\right)^2\right]\\
&=nlog\left(\frac{\sigma}{\sigma_0}\right)-\frac{n}{2} + nE\left[\frac{1}{2}\left(\frac{y-\mu}{\sigma}\right)^2\right]\\
&=nlog\left(\frac{\sigma}{\sigma_0}\right)-\frac{n}{2} + \frac{n}{2\sigma^2}E\left[y^2+\mu^2-2y\mu\right]\\
&=nlog\left(\frac{\sigma}{\sigma_0}\right)-\frac{n}{2} + n\frac{\sigma_0^2+\mu_0^2}{2\sigma^2} + n\frac{\mu^2}{2\sigma^2}+n\frac{2\mu_0\mu}{2\sigma^2}\\
&=nlog\left(\frac{\sigma}{\sigma_0}\right)-\frac{n}{2}\left(1-\frac{\sigma_0^2}{\sigma^2}\right)  + \frac{n}{2}\left(\frac{\mu-\mu_0}{\sigma}\right)^2.\\
\end{align*}

## (3)

By solving the first order condition of the Kullback-Liebler divergence,

$$\frac{\partial K_n(\boldsymbol{\theta}:\boldsymbol{\theta}_0)}{\partial\mu} = \frac{n\mu-n\mu_0}{\sigma^2}=0.$$

$$\frac{\partial K_n(\boldsymbol{\theta}:\boldsymbol{\theta}_0)}{\partial\sigma} = n\sigma^{-1}-n\sigma^{-3}(\sigma^2_0+n(\mu-\mu_0)) =0$$

We have $\hat{\mu}=\mu_0$ and $\hat{\sigma} = \sigma_0$.

The fisher information per observation is

$$
\begin{align*}
\boldsymbol{i}(\boldsymbol{\theta}_0) &= \left. -E\left(\frac{\partial^2 log f(y:\boldsymbol{\theta})}{\partial\boldsymbol{\theta}\partial\boldsymbol{\theta}'}\right) \right|_{\boldsymbol{\theta} = \boldsymbol{\theta}_0} \\
&=\left. -E\left(\begin{bmatrix}\frac{\partial^2 log f(y:\boldsymbol{\theta})}{\partial\mu^2} & \frac{\partial^2 log f(y:\boldsymbol{\theta})}{\partial\mu\partial\sigma}\\\frac{\partial^2 log f(y:\boldsymbol{\theta})}{\partial\sigma\partial\mu} & \frac{\partial^2 log f(y:\boldsymbol{\theta})}{\partial\sigma^2}\end{bmatrix}\right)\right|_{\boldsymbol{\theta} = \boldsymbol{\theta}_0}\\
&=\left. -E\left(\begin{bmatrix} -\sigma^{-2} & (2\mu-2y)\sigma^{-3}\\(2\mu-2y)\sigma^{-3} & \sigma^{-2}-3(y-\mu)^2\sigma^{-4}\end{bmatrix}\right)\right|_{\boldsymbol{\theta} = \boldsymbol{\theta}_0}\\
&=-\begin{bmatrix} -\sigma_0^{-2} & 0 \\ 0 & -2\sigma_0^{-2}\end{bmatrix}\\
&=\begin{bmatrix} \sigma_0^{-2} & 0 \\ 0 & 2\sigma_0^{-2}\end{bmatrix}.\\
\end{align*}
$$

## (4)

Using the invariance property, the MLE of $\boldsymbol{\theta}$ is $$\hat{\boldsymbol{\theta}} = (\sum_{i=1}^{n}y_i/n,\sqrt{\sum_{i=1}^{n}\{y_i-(\sum_{i=1}^{n}y_i/n)\}^2/n})'.$$

The variance of this estimator is 

$$
\begin{align*}
var(\hat{\boldsymbol{\theta}}) &=\begin{bmatrix} var(\hat{\mu}) & cov(\hat{\mu}\hat{\sigma})\\ cov(\hat{\sigma}\hat{\mu}) & var(\hat{\sigma}) \end{bmatrix}\\
&=\begin{bmatrix} \sigma_0^2/n & 0\\ 0 & \sigma_0^2\left(1-\frac{2}{n}\left\{\frac{\Gamma((n+1)/2)}{\Gamma(n/2)}\right\}^2\right) \end{bmatrix}.\\
\end{align*}
$$
The CRLB is $$(n\boldsymbol{i}(\boldsymbol{\theta}_0))^{-1} = \begin{bmatrix} \sigma_0^2/n & 0\\ 0 & \sigma_0^2/(2n) \end{bmatrix}.$$ This is smaller than the variance of the MLE.

# Question 3

## (1)

The likelihood for $\theta$ in the first case is 
$$L(\theta:y_1,..y_n) = K(y_1,..y_n) \times f(y_1,..y_n:\theta)=K(y_1,..y_n) \times \prod_{i=1}^{n}\frac{1}{\sqrt{2\pi\theta}}exp\left(-\frac{(y_i-\theta)^2}{2\theta}\right).$$
The support is 
$$logL(\theta:y_1,..y_n) = logK(y_1,..y_n) + logf(y_1,..y_n:\theta)=k + \sum_{i=1}^{n}-\frac{1}{2}log(\theta) -\frac{(y_i-\theta)^2}{2\theta}$$

The score function is 
$$\frac{\partial logL(\theta:y_1,..y_n)}{\partial \theta} = \sum_{i=1}^{n}-\frac{1}{2\theta}+\frac{y_i^2}{2\theta^2}-\frac{1}{2}.$$


The likelihood for $\theta$ in the second case is 

$$L(\theta:y_1,..y_n) = K(y_1,..y_n) \times f(y_1,..y_n:\theta)=K(y_1,..y_n) \times \prod_{i=1}^{n}\frac{y_i}{\theta^2}exp(-y_i^2/(2\theta^2)).$$
The support is 
$$logL(\theta:y_1,..y_n) = logK(y_1,..y_n) \times logf(y_1,..y_n:\theta)=k + \sum_{i=1}^{n}log(y_i)-2log(\theta) -y_i^2/(2\theta^2).$$

The score function is $$\frac{\partial logL(\theta:y_1,..y_n)}{\partial \theta} =\sum_{i=1}^{n}-\frac{2}{\theta}+\frac{y_i^2}{\theta^3}.$$

## (2)

The expectation of the score function will be equal to zero when evaluated at $\theta_0$. 

For the first case:

$$E[\sum_{i=1}^{n}-\frac{1}{2\theta}+\frac{y_i^2}{2\theta^2}-\frac{1}{2}]|_{\theta=\theta_0}=-\frac{n}{2\theta_0}-\frac{n}{2}+\frac{n(\theta_0^2 + \theta_0)}{2\theta_0^2}=0.$$


For the second case:

$$E[\sum_{i=1}^{n}-\frac{2}{\theta}+\frac{y_i^2}{\theta^3}]|_{\theta=\theta_0}=-\frac{n}{2\theta_0}-\frac{n\{(\theta\sqrt{\pi/2})^2 + 2\theta^2(1-\pi/4)\}}{\theta_0^3}=0.$$
## (3)

For the first case:

$$
\begin{align*}
i(\theta) &= -E\left.\left(\frac{\partial logf(y:\theta)}{\partial \theta^2}\right)\right|_{\theta=\theta_0}\\
&= -E\left.\left(\frac{1}{2\theta^2}-\frac{y^2}{\theta^3}\right)\right|_{\theta=\theta_0}\\
&= -\frac{1}{2\theta_0^2}+\frac{\theta_0^2+\theta_0}{\theta_0^3}\\
&= \frac{2\theta_0^2+\theta_0}{2\theta_0^3}.\\
\end{align*}
$$

For the second case:

$$
\begin{align*}
i(\theta) &= -E\left.\left(\frac{\partial logf(y:\theta)}{\partial \theta^2}\right)\right|_{\theta=\theta_0}\\
&= -E\left.\left(\frac{2}{\theta^2}-\frac{3y^2}{\theta^4}\right)\right|_{\theta=\theta_0}\\
&= -\frac{2}{\theta_0^2}+\frac{3\{(\theta_0\sqrt{\pi/2})^2+2\theta_0^2(1-\pi/4)\}}{\theta_0^4}\\
&= -\frac{2}{\theta_0^2}+\frac{6}{\theta_0^2}\\
&= \frac{4}{\theta_0^2}.\\
\end{align*}
$$

# Question 4

## (1)

The joint pdf of $y$ and $\boldsymbol{x}$ is 

$$f(y,\boldsymbol{x}:\boldsymbol{\beta},\boldsymbol{\theta}) = f(y|\boldsymbol{x}:\boldsymbol{\beta})f(\boldsymbol{x}:\boldsymbol{\theta}).$$
The conditional log likelihood is 

$$
\begin{align*}
logL(\boldsymbol{\beta}:y_1,...,y_n)&=log\prod_{i=1}^{n}f(y_i|\boldsymbol{x}_i:\boldsymbol{\beta})\\
&=\sum_{i=1}^{n}logf(y_i|\boldsymbol{x}_i:\boldsymbol{\beta})\\
&=\sum_{i=1}^{n}log\frac{exp(-exp(\boldsymbol{\beta}'\boldsymbol{x}_i))exp(\boldsymbol{\beta}'\boldsymbol{x}_i)^{y_i}}{y_i!}\\
&=\sum_{i=1}^{n}-exp(\boldsymbol{\beta}'\boldsymbol{x}_i)+y_i\boldsymbol{\beta}'\boldsymbol{x}_i-log(y_i!).\\
\end{align*}
$$

## (2) and (3)

We can obtain the MLE by solving the first order condition

$$\frac{\partial logL(\boldsymbol{\beta}:y_1,...,y_n)}{\partial \boldsymbol{\beta}} = \sum_{i=1}^{n}-exp(\boldsymbol{\beta}'\boldsymbol{x}_i)\boldsymbol{x_i}+y_i\boldsymbol{x}_i =\sum_{i=1}^{n}\boldsymbol{x}_i[-exp(\boldsymbol{\beta}'\boldsymbol{x}_i)+y_i]=\boldsymbol{0}$$

We could also check the second order condition

$$\frac{\partial^2 logL(\boldsymbol{\beta}:y_1,...,y_n)}{\partial \boldsymbol{\beta}\partial\boldsymbol{\beta}'}  =\sum_{i=1}^{n}-exp(\boldsymbol{\beta}'\boldsymbol{x}_i)\boldsymbol{x_i}\boldsymbol{x_i}'$$

The diagonal terms of this matrix is negative, which suggests this matrix is a negative definite matrix. It is a maximum.


By Proposition 5.6 from the lecture note, if $\Theta$ is a compact subset of $\mathbb{R}^{k}$ and the support is a continuous function of $\boldsymbol{\theta}$ then the MLE exists. Moreover, if the support function is also differentiable in $\boldsymbol{\theta}$ and the MLE $\hat{\boldsymbol{\theta}}$ is interior to $\Theta$ then

$$S(\hat{\boldsymbol{\theta}}:z_1,...,z_n) = S(\boldsymbol{\theta}:z_1,...,z_n)|_{\boldsymbol{\theta}=\hat{\boldsymbol{\theta}}}=\boldsymbol{0}.$$

In this case, if we set the parameter space $\Theta$ to be a compact subset of $\mathbb{R}^{k}$. Then, because the support is a continuous function of $\boldsymbol{\beta}$, the MLE exists. Moreover, if we assume the MLE $\hat{\boldsymbol{\beta}}$ is interior to $\Theta$, then since the support function is differentiable in $\boldsymbol{\beta}$, the MLE can be obtained by solving the first order condition.


## (4)

To be able to use the WLLN(Khinchine), $\boldsymbol{z}_i = \boldsymbol{x}_i[y_i-exp(\boldsymbol{\beta}'\boldsymbol{x}_i)]$ needs to be iid,  $E[\boldsymbol{z}_i]<\boldsymbol{\infty}$, and $var[\boldsymbol{z}_i]<\boldsymbol{\infty}$.

Hence, $\boldsymbol{x}_i$ needs to be iid, and the first two moments of $\boldsymbol{x}_i$ does exist.

By the WLLN,

$$\frac{1}{n}\sum_{i=1}^{n}\boldsymbol{x}_i[y_i-exp(\boldsymbol{\beta}'\boldsymbol{x}_i)] \overset{p}{\longrightarrow}E[\boldsymbol{x}\{y-exp(\boldsymbol{\beta}'\boldsymbol{x})\}].$$

We need conditions (i) to (v) from the lecture note to argue that $\hat{\boldsymbol{\beta}}$ is a consistent estimator of $\boldsymbol{\beta}$.






