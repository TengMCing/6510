---
title: "ex5"
output: 
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Question 1

## (1)

The support is $$
\begin{align}
logL(\boldsymbol{\theta}:\boldsymbol{y}) &= logZ(\boldsymbol{y}) + logf(\boldsymbol{y}:\boldsymbol{\theta}) \\
&= logZ(\boldsymbol{y}) + log\prod_{i=1}^{n}f(y_i:\boldsymbol{\theta})\\
&= logZ(\boldsymbol{y}) + \sum_{i=1}^{n}logf(y_i:\boldsymbol{\theta})\\
&= logZ(\boldsymbol{y}) + \sum_{i=1}^{n}-\frac{1}{2}log(2\pi\sigma^2)-\frac{1}{2}\frac{(y_i-\mu)^2}{\sigma^2}.\\
\end{align}
$$

The score function is

$$
\begin{align*}
\frac{\partial logL(\boldsymbol{\theta}:\boldsymbol{y})}{\partial\boldsymbol{\theta}} &= \begin{bmatrix}\sum_{i=1}^{n} -\frac{\mu-y_i}{\sigma^2} \\ \sum_{i=1}^{n} -\frac{1}{2\sigma^2} + \frac{(y_i-\mu)^2}{2\sigma^4} \end{bmatrix}.
\end{align*}
$$

The Hessian is

$$
\begin{align*}
\frac{\partial^2 logL(\boldsymbol{\theta}:\boldsymbol{y})}{\partial\boldsymbol{\theta}\partial\boldsymbol{\theta}'} &= \begin{bmatrix}\sum_{i=1}^{n} -\frac{1}{\sigma^2} &  \sum_{i=1}^{n} \frac{\mu-y_i}{\sigma^4} \\ \sum_{i=1}^{n} \frac{\mu-y_i}{\sigma^4} & \sum_{i=1}^{n} \frac{1}{2\sigma^4} - \frac{(y_i-\mu)^2}{\sigma^6} \end{bmatrix}.
\end{align*}
$$

The information matrix is $$
\begin{align*}
I(\boldsymbol{\theta}) &= -E\left[\frac{\partial^2 logL(\boldsymbol{\theta}:\boldsymbol{y})}{\partial\boldsymbol{\theta}\partial\boldsymbol{\theta}'}\right]\\
&= -E\begin{bmatrix}\sum_{i=1}^{n} -\frac{1}{\sigma^2} &  \sum_{i=1}^{n} \frac{\mu-y_i}{\sigma^4} \\ \sum_{i=1}^{n} \frac{\mu-y_i}{\sigma^4} & \sum_{i=1}^{n} \frac{1}{2\sigma^4} - \frac{(y_i-\mu)^2}{\sigma^6} \end{bmatrix}\\
&= \begin{bmatrix} \frac{n}{\sigma^2_0} & 0 \\ 0 &  \frac{n}{2\sigma_0^4}\end{bmatrix}.
\end{align*}
$$

## (2)

### (a)

$$h(\boldsymbol{\theta}) = (\mu-\mu_0,0\times\sigma^2)' = (0,0)'.$$

$$D(\boldsymbol{\theta}) = \frac{\partial h(\boldsymbol{\theta})}{\partial \boldsymbol{\theta}'}=\begin{bmatrix}1 & 0 \\ 0 & 0\end{bmatrix}.$$

The Wald test statistic is $$
\begin{align*}
TW_n &= \left.nh(\boldsymbol{\theta})'\left[D(\boldsymbol{\theta})\widehat{i(\boldsymbol{\theta})}^{-1}D(\boldsymbol{\theta})'\right]^{-1}h(\boldsymbol{\theta})\right|_{\boldsymbol{\theta} = \hat{\boldsymbol{\theta}}} \\
&= n\begin{bmatrix}\hat{\mu}-\mu_0 & 0\end{bmatrix}\left[\begin{bmatrix}1 & 0 \\ 0 & 0\end{bmatrix}\begin{bmatrix}\hat{\sigma}^2 & 0 \\ 0 & \hat{\sigma}^4/2\end{bmatrix}\begin{bmatrix}1 & 0 \\ 0 & 0\end{bmatrix}\right]^{-1}\begin{bmatrix}\hat{\mu}-\mu_0 \\ 0\end{bmatrix} \\
&= n\begin{bmatrix}\hat{\mu}-\mu_0 & 0\end{bmatrix}\begin{bmatrix}\hat{\sigma}^{-2} & 0 \\ 0 & 0\end{bmatrix}\begin{bmatrix}\hat{\mu}-\mu_0 \\ 0\end{bmatrix} \\
&= n\hat{\sigma}^{-2}(\hat{\mu}-\mu_0)^2 \\
&= n\frac{(\bar{y}-\mu_0)^2}{\frac{1}{n}\sum_{i=1}^{n}(y_i - \bar{y})^2}. \\
\end{align*}
$$

Given $\tilde{\mu} = \mu_0$, the restricted MLE $\tilde{\sigma}^2 = \frac{1}{n}\sum_{i=1}^{n}(y_i-\mu_0)^2$.

The LM test statistic is

$$
\begin{align*}
TS_n &= n^{-1}S(\tilde{\boldsymbol{\theta}})'\widehat{i(\tilde{\boldsymbol{\theta}})}^{-1}S(\tilde{\boldsymbol{\theta}}) \\
&= n^{-1}S(\theta_0)'\widehat{i(\boldsymbol{\theta}_0)}^{-1}S(\theta_0)\\
&= n^{-1}\begin{bmatrix}\sum_{i=1}^{n} -\frac{\mu_0-y_i}{\tilde{\sigma}^2} \\ 0 \end{bmatrix}'\begin{bmatrix}\tilde{\sigma}^2 & 0 \\ 0 & \tilde{\sigma}^4/2\end{bmatrix}\begin{bmatrix}\sum_{i=1}^{n} -\frac{\mu_0-y_i}{\tilde{\sigma}^2} \\ 0 \end{bmatrix}\\
&= n^{-1}\begin{bmatrix}\sum_{i=1}^{n} -\mu_0+y_i \\ 0 \end{bmatrix}'\begin{bmatrix}\sum_{i=1}^{n} -\frac{\mu_0-y_i}{\tilde{\sigma}^2} \\ 0 \end{bmatrix}\\
&= n\frac{(\bar{y}-\mu_0)^2}{\frac{1}{n}\sum_{i=1}^{n}(y_i-\mu_0)^2}. \\
\end{align*}
$$ The likelihood ratio test statistic is $$
\begin{align}
TLR_n &= 2[logL(\hat{\theta}) - logL(\tilde{\theta})]\\
&= 2\left[\sum_{i=1}^{n}-\frac{1}{2}log(2\pi\hat{\sigma}^2)-\frac{1}{2}\frac{(y_i-\hat{\mu})^2}{\hat{\sigma}^2} - \sum_{i=1}^{n}-\frac{1}{2}log(2\pi\tilde{\sigma}^2)-\frac{1}{2}\frac{(y_i-\tilde{\mu})^2}{\tilde{\sigma}^2}\right]\\
&= \sum_{i=1}^{n}-log(\hat{\sigma}^2)-\frac{(y_i-\hat{\mu})^2}{\hat{\sigma}^2} - \sum_{i=1}^{n}-log(\tilde{\sigma}^2)-\frac{(y_i-\tilde{\mu})^2}{\tilde{\sigma}^2}\\
&= \sum_{i=1}^{n}-log(\hat{\sigma}^2) - \sum_{i=1}^{n}-log(\tilde{\sigma}^2)\\
&= \sum_{i=1}^{n}log(\tilde{\sigma}^2/\hat{\sigma}^2)\\
&= nlog\left(\frac{\sum_{i=1}^{n}(y_i-\mu_0)^2}{\sum_{i=1}^{n}(y_i-\bar{y})^2}\right).\\
\end{align}
$$

## (3)

$$
\begin{align*}
log(x) &\leq x-1 \\
-log(x) &\geq 1-x\\
TLR_n &=nlog\left(\frac{\sum_{i=1}^{n}(y_i-\mu_0)^2}{\sum_{i=1}^{n}(y_i-\bar{y})^2}\right) \\
&= -nlog\left(\frac{\sum_{i=1}^{n}(y_i-\mu_0)^2}{\sum_{i=1}^{n}(y_i-\bar{y})^2}\right) \\
&\geq n\left(1-\frac{\sum_{i=1}^{n}(y_i-\mu_0)^2}{\sum_{i=1}^{n}(y_i-\bar{y})^2}\right) \\
&= n\left(\frac{\sum_{i=1}^{n}(y_i-\bar{y})^2-\sum_{i=1}^{n}(y_i-\mu_0)^2}{\sum_{i=1}^{n}(y_i-\bar{y})^2}\right) \\
&= n\left(\frac{n(\bar{y}-\mu_0)^2}{\sum_{i=1}^{n}(y_i-\bar{y})^2}\right) \\
&= n\frac{(\bar{y}-\mu_0)^2}{\frac{1}{n}\sum_{i=1}^{n}(y_i-\mu_0)^2}\\
&= TS_n.
\end{align*}
$$

$$
\begin{align*}
log(x) &\leq x-1 \\
TLR_n &=nlog\left(\frac{\sum_{i=1}^{n}(y_i-\mu_0)^2}{\sum_{i=1}^{n}(y_i-\bar{y})^2}\right) \\
&\leq n\left(\frac{\sum_{i=1}^{n}(y_i-\mu_0)^2}{\sum_{i=1}^{n}(y_i-\bar{y})^2}-1\right) \\
&= n\left(\frac{\sum_{i=1}^{n}(y_i-\mu_0)^2-\sum_{i=1}^{n}(y_i-\bar{y})^2}{\sum_{i=1}^{n}(y_i-\bar{y})^2}\right) \\
&= n\left(\frac{n(\bar{y}-\mu_0)^2}{\sum_{i=1}^{n}(y_i-\bar{y})^2}\right) \\
&= n\frac{(\bar{y}-\mu_0)^2}{\frac{1}{n}\sum_{i=1}^{n}(y_i-\bar{y})^2}\\
&= TW_n.
\end{align*}
$$

# Question 2

## (1)

We can express the model as:

$$\begin{bmatrix}\boldsymbol{y}_1 \\ \boldsymbol{y}_2 \end{bmatrix} = \begin{bmatrix}\boldsymbol{X}_1 & 0 \\ 0 &  \boldsymbol{X}_2 \end{bmatrix}\begin{bmatrix}\boldsymbol{\beta}_1 \\ \boldsymbol{\beta}_2 \end{bmatrix} + \begin{bmatrix}\boldsymbol{\varepsilon}_1 \\ \boldsymbol{\varepsilon}_2 \end{bmatrix},$$

where $\boldsymbol{\varepsilon}_1 \sim N(0,\sigma^2)$ and $\boldsymbol{\varepsilon}_1 \sim N(0,\sigma^2)$.

Let $\boldsymbol{y} = \begin{bmatrix}\boldsymbol{y}_1 \\ \boldsymbol{y}_2 \end{bmatrix}$, $\boldsymbol{X} = \begin{bmatrix}\boldsymbol{X}_1 & 0 \\ 0 & \boldsymbol{X}_2 \end{bmatrix}$, and $\boldsymbol{\beta} = \begin{bmatrix}\boldsymbol{\beta}_1 \\ \boldsymbol{\beta}_2 \end{bmatrix}$.

By setting the score function equals to zero,

$$\frac{\partial logL(\boldsymbol{\beta},\sigma^2:\boldsymbol{y})}{\partial\boldsymbol{\beta}} = \frac{\boldsymbol{X}'(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta})}{\sigma^2}=0$$ $$\frac{\partial logL(\boldsymbol{\beta},\sigma^2:\boldsymbol{y})}{\partial\sigma^2} = -\frac{n}{2\sigma^2}+\frac{(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta})'(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta})}{2\sigma^4}=0$$

Since $\boldsymbol{X}'\boldsymbol{X}$ is nonsingular, we have $\hat{\boldsymbol{\beta}}=(\boldsymbol{X}'\boldsymbol{X})^{-1}\boldsymbol{X}'\boldsymbol{y}$ and $\hat{\sigma^2} = (\boldsymbol{y}-\boldsymbol{X}\hat{\boldsymbol{\beta}})'(\boldsymbol{y}-\boldsymbol{X}\hat{\boldsymbol{\beta}})n^{-1}$.

The maximum log-likelihood is $$
\begin{align*}
logL(\boldsymbol{y}:\hat{\boldsymbol{\beta}},\hat{\sigma}|\boldsymbol{X}) &= -\frac{n}{2}log(2\pi\hat{\sigma}^2)-\frac{(\boldsymbol{y}-\boldsymbol{X}\hat{\boldsymbol{\beta}})'(\boldsymbol{y}-\boldsymbol{X}\hat{\boldsymbol{\beta}})}{2\hat{\sigma}^2}\\
&=-\frac{n}{2}log(2\pi \boldsymbol{e}'\boldsymbol{e}n^{-1})-\frac{\boldsymbol{e}'\boldsymbol{e}}{2\boldsymbol{e}'\boldsymbol{e}n^{-1}}\\
&=-\frac{n}{2}log[2\pi(\boldsymbol{e}_1'\boldsymbol{e}_1 + \boldsymbol{e}_2'\boldsymbol{e}_2)n^{-1}]-\frac{n}{2}.\\
\end{align*}
$$

## (2)

We have derived the solution in (1), though the dimension of $\boldsymbol{\beta}$ is now only $k\times 1$, and the dimension of $\boldsymbol{X}$ is now $n\times k$, the solution is still hold. The log-likelihood is $$logL(\boldsymbol{y}:\hat{\boldsymbol{\beta}},\hat{\sigma}|\boldsymbol{X}) = -\frac{n}{2}log(2\pi\hat{\sigma}^2)-\frac{(\boldsymbol{y}-\boldsymbol{X}\hat{\boldsymbol{\beta}})'(\boldsymbol{y}-\boldsymbol{X}\hat{\boldsymbol{\beta}})}{2\hat{\sigma}^2} = -\frac{n}{2}log(2\pi\boldsymbol{e}_0'\boldsymbol{e}_0n^{-1})-\frac{\boldsymbol{e}_0'\boldsymbol{e}_0}{2\boldsymbol{e}_0'\boldsymbol{e}_0n^{-1}}=-\frac{n}{2}log(2\pi\boldsymbol{e}_0'\boldsymbol{e}_0n^{-1})-\frac{n}{2}.$$

## (3)

The LR test statistic is

$$TLR_n = 2[logL(\boldsymbol{\theta}_U) - logL(\boldsymbol{\theta}_R)]=-nlog[2\pi(\boldsymbol{e}_1'\boldsymbol{e}_1 + \boldsymbol{e}_2'\boldsymbol{e}_2)n^{-1}]+nlog(2\pi\boldsymbol{e}_0'\boldsymbol{e}_0n^{-1})=nlog\left(\frac{\boldsymbol{e}_0'\boldsymbol{e}_0}{\boldsymbol{e}_1'\boldsymbol{e}_1 + \boldsymbol{e}_2'\boldsymbol{e}_2}\right).$$

We reject the null hypothesis when $TLR_n > \mathcal{X}^2_{1-\alpha}(k)$.

We can see that we reject the null hypothesis when the unrestricted RSS is significantly smaller than the restricted RSS.

The chow test uses the test statistic $$\lambda = \frac{n-2k}{k}\left(\frac{\boldsymbol{e}_0'\boldsymbol{e}_0}{\boldsymbol{e}_1'\boldsymbol{e}_1 + \boldsymbol{e}_2'\boldsymbol{e}_2}-1\right).$$

# Question 3

## (1)

The information matrix is

$$i(\theta) = \left.-E\left[\frac{d^2logL(\theta:y)}{d\theta^2}\right]\right|_{\theta = \theta_0}=\begin{bmatrix}\frac{1}{\theta_0^2}\end{bmatrix}$$

$$h(\theta) = \begin{bmatrix}\theta-\theta_0\end{bmatrix}.$$

$$D(\theta) = \frac{dh(\theta)}{d\theta}=[1].$$

$$
\begin{align*}
TW_n &= \left.nh(\theta)'\left[D(\theta)\widehat{i(\theta)}^{-1}D(\theta)'\right]^{-1}h(\theta)\right|_{\theta = \hat{\theta}} \\
&= n\begin{bmatrix}\hat{\theta}-\theta_0\end{bmatrix}'\begin{bmatrix}\frac{1}{\hat{\theta}^2}\end{bmatrix}\begin{bmatrix}\hat{\theta}-\theta_0\end{bmatrix} \\
&= n(\hat{\theta}-\theta_0)^2\hat{\theta}^{-2}.
\end{align*}
$$

## (2)

$$
\begin{align*}
TLR_n &= 2[logL(\hat{\theta}_U) - logL(\hat{\theta}_R)] \\
&= 2\left[\sum_{i=1}^{n}log(\theta_0/\hat{\theta})-\frac{y_i}{\hat{\theta}}+\frac{y_i}{\theta_0}\right]\\
&= 2nlog(\theta_0/\hat{\theta})-2n+2n\bar{y}/\theta_0.
\end{align*}
$$

## (3)

$$
\begin{align*}
TS_n &= n^{-1}S(\tilde{\theta})'\widehat{i(\tilde{\theta})}^{-1}S(\tilde{\theta}) \\
&= n^{-1} (\sum_{i=1}^{n} -\theta_0^{-1} +y_i\theta_0^{-2} )^2\theta_0^2\\
&= n(-\theta_0^{-1} + \bar{y}\theta_0^{-2} )^2\theta_0^2.\\
\end{align*}
$$

## (4)

Wald

```{r}
n <- 40
theta_0 <- 1
theta_hat <- 1.25
n*(theta_hat-theta_0)^2*theta_hat^(-2)
```

LR

```{r}
2*n*log(theta_0/theta_hat)-2*n+2*n*theta_hat/theta_0
```

LM

```{r}
n*(-theta_0^(-1)+theta_hat*theta_0^(-2))^2*theta_0^2
```

# Question 4

## (1)

The pdf of $X$ is $f(x) = [qp + (1-p)(1-q)]^x [(1-q)p + q(1-p)]^{(1-x)}$, $X \in \{0,1\}$.

The mean of $X$ is $E[X] = qp + (1-p)(1-q)$.

The variance of $X$ is $V[X] = E[X^2] - (E[X])^2 = qp + (1-p)(1-q) - [qp + (1-p)(1-q)]^2$.

## (2)

Let $g = qp + (1-p)(1-q)$, and $1-g = 1- qp - (1-p)(1-q)$.

By solving the first order condition of the log-likelihood, we obtain the MLE of $g$

$$\frac{d logL(g,\boldsymbol{X})}{dg} = \sum_{i=1}^{n}x/g+(1-x)/(1-g)=0$$ $$\hat{g} = \bar{x}.$$

By the invariance of MLE, since $g = qp + (1-p)(1-q)$ which is a one-to-one mapping, the MLE of $p$ will be $\hat{p} = (\bar{x}+q-1)/(2q-1)$.

## (3)

The mean of $\hat{p}$ is $E[\hat{P}] = E[(\bar{x}+q-1)/(2q-1)] = \frac{qp + (1-p)(1-q) + q - 1}{2q-1} = p$.

The variance of $\hat{p}$ is $V[\hat{P}] = V[(\bar{x}+q-1)/(2q-1)] = V[\bar{x}]/(2q-1)^2 = nV[x]/[(2q-1)n]^2 = g(1-g)/[n(2q-1)^2]$.

$V[\hat{P}] = g(1-g)/[n(2q-1)^2] \overset{p}{\rightarrow}0$ as $n \rightarrow \infty$.

## (4)

The information per observation is

$$
\begin{align*}
E\left[\frac{d^2 logL(p,\boldsymbol{x})}{dp^2}\right] &= E\left[\sum_{i=1}^{n}\frac{d}{dp}\frac{(2q-1)x}{qp + (1-p)(1-q)} + \frac{(1-2q)(1-x)}{(1-q)p+q(1-p)}\right] \\
&= E\left[\sum_{i=1}^{n}-\frac{(2q-1)^2x}{[qp+(1-p)(1-q)]^2}-\frac{(1-x)(1-2q)^2}{[(1-q)p+q(1-p)]^2}\right]\\
&= E\left[-(2q-1)^2\left[\frac{\sum_{i=1}^{n}x}{g^2}+\frac{\sum_{i=1}^{n}1-x}{(1-g)^2}\right]\right] \\
&= -(2q-1)^2(g^{-1}+(1-g)^{-1})\\
&= -(2q-1)^2[g(1-g)]^{-1}\\
&= V[\hat{P}]\times n.
\end{align*}
$$

## (5)

```{r}
x_bar <- 0.3
q <- 0.25
(x_bar+q-1)/(2*q-1)
```

## (6)

If the coin is not biased, the estimator is not well-defined.
